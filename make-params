#!/usr/bin/env python

import itertools
import json

# Download the model-training code from this S3 URI.
model_s3_uri = 's3://nad-sandbox/jobs/Semantic-Segmentation-Suite/code'


# Train the given model architecture.
models = [
    'FC-DenseNet56', 'Encoder-Decoder-Skip',
    'RefineNet-Res50', 'FRRN-A', 'FRRN-B',
    'MobileUNet-Skip', 'PSPNet-Res50', 'GCN-Res50', 'DeepLabV3-Res50',
    'DeepLabV3_plus-Res50', 'AdapNet'
]

# Using this learning rate.
learning_rates = [0.003, 0.001, 0.0003, 0.0001]

# And dataset.
dataset_name = 'dataset-HT'

output = []
iter = itertools.product(models, learning_rates)

for model_name, learning_rate in iter:
    d = {
        'model-name': model_name,
        'learning-rate': learning_rate,
        'model-s3-uri': model_s3_uri,
        'dataset-name': dataset_name
    }

    output.append(d)

print(json.dumps(output, indent=2))
